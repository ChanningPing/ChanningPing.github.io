---
title: Projects
---
  <div class="container">
  </div>

  <div class="project-container">
    <h1>Projects</h1>
    <p>Some of the things I've dabbled on.</p>


  </div>

  <div class="container">
    <div class="project-box">
      <div class="row">
        <div class="col-md-3 project-image">
          <img hspace="20" vspace="23" src="/images/videohighlights.png">
        </div>
        <div class="col-md-9 project-post">
          <h3>Video Highlight Detection with Time-sync Comments</h3><h4><a href="https://arxiv.org/abs/1708.02210">paper</a> | <a href="https://www.dropbox.com/s/i1pfp9g8ur1hczr/Qing_LitStoryTeller.pdf?dl=0">slides</a> | <a href="https://github.com/ChanningPing/VideoHighlightDetection">data</a> | more details</h4>
          <p class="meta"><small>&nbsp;<i class="fa fa-calendar-o"></i> 2017</small></p><hr/>
          <div class="post">
          This project aims to predict highlight shots of a video using frame-by-frame time-sync comments in a unsupervised fashion. We also perform a secondary task of highlight text summarization using concept-downsampled SumBasic. To predict a highlight shot, we consider (1) comment volume; (2) emotional concentration in the comments; and (3) topical concentration in the comments of a shot. Before representation of a video, we also perform lag-calibration by (1) constructing a concept mapping dictionary built in a global word-embedding space; (2) constructing lexical chains of sentences and (3) calibrate sentence timestamps based on lexical chains. Experimental results suggest the significant contribution of lag-calibration, as well as the modeling of highlights with comment volume, emotion concentration and topic concentration in highlight prediction.
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="container">
    <div class="project-box">
      <div class="row">
        <div class="col-md-3 project-image">
          <img hspace="20" vspace="23" src="/images/storyteller.png">
        </div>
        <div class="col-md-9 project-post">
          <h3>LitStoryTeller: An Interactive System for Visual Exploration of Scientific Papers</h3><h4><a href="https://arxiv.org/abs/1708.02214">paper</a> |  <a href="https://www.dropbox.com/s/6lkao955gd987cf/Qing_highlight_summarization.pdf?dl=0">slides</a> | more details</h4>
          <p class="meta"><small>&nbsp;<i class="fa fa-calendar-o"></i> 2013</small></p><hr/>
          <div class="post">
          The present study proposes LitStoryTeller, an interactive system for visually exploring the semantic structure of a scientific article. The proposed system borrows a metaphor from screen play, and visualizes the storyline of a scientific paper by arranging its characters (scientific concepts or terminologies) and scenes (paragraphs/sentences) into a progressive and interactive storyline.
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="container">
    <div class="project-box">
      <div class="row">
        <div class="col-md-3 project-image">
          <img hspace="20" vspace="23" src="/images/user_event_path.png">
        </div>
        <div class="col-md-9 project-post">
          <h3>How many ways to use CiteSpace? A study of user interactive events over 14 months</h3><h4><a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.23770/full">paper</a> | more details</h4>
          <p class="meta"><small>&nbsp;<i class="fa fa-calendar-o"></i> 2013</small></p><hr/>
          <div class="post">
          In this project, we analyze millions of interactive events in logs of <a href="http://cluster.cis.drexel.edu/~cchen/citespace/">Citespace</a> generated by users worldwide over a 14-month period. The key findings are: (i) three levels of proficiency are identified, namely, level 1: low proficiency, level 2: intermediate proficiency, and level 3: high proficiency, and (ii) behavioral patterns at level 3 are resulted from a more engaging interaction with the system.
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="project-box">
      <div class="row">
        <div class="col-md-3 project-image">
          <img hspace="20" vspace="23" src="/images/user_event_path.png">
        </div>
        <div class="col-md-9 project-post">
          <h3>Breast Cancer Symptom Clusters Derived From Social Media and Research Study Data Using Improved K-Medoid Clustering</h3><h4><a href="http://ieeexplore.ieee.org/abstract/document/7737067/">paper</a> | more details</h4>
          <p class="meta"><small>&nbsp;<i class="fa fa-calendar-o"></i> 2013</small></p><hr/>
          <div class="post">
          Social media, such as online healthrelated forums, could be used as a data source to identify and characterize symptom clusters among cancer patients. This paper seeks to determine patterns of symptom clusters in breast cancer survivors derived from both social media and research study data using improved K-medoid clustering.
          </div>
        </div>
      </div>
    </div>
  </div>

</div>
